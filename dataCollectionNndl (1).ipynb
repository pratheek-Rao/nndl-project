{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541dd163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\prath\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Skipping non-image file: data\\0\\.ipynb_checkpoints\n",
      "Skipping non-image file: data\\1\\.ipynb_checkpoints\n",
      "WARNING:tensorflow:From C:\\Users\\prath\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\prath\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\prath\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\prath\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\prath\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "8/8 [==============================] - 2s 111ms/step - loss: 2.0496 - accuracy: 0.2305 - val_loss: 1.9526 - val_accuracy: 0.1875\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.6294 - accuracy: 0.4648 - val_loss: 1.3053 - val_accuracy: 0.5781\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.8924 - accuracy: 0.7852 - val_loss: 0.4039 - val_accuracy: 0.9844\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.2878 - accuracy: 0.9297 - val_loss: 0.1075 - val_accuracy: 0.9844\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.1274 - accuracy: 0.9609 - val_loss: 0.0847 - val_accuracy: 0.9844\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0488 - accuracy: 0.9922 - val_loss: 0.0707 - val_accuracy: 0.9844\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 8.5185e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 7.7576e-04 - accuracy: 1.0000 - val_loss: 5.9163e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 5.0011e-04 - accuracy: 1.0000 - val_loss: 4.0928e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 4.2357e-04 - accuracy: 1.0000 - val_loss: 3.1472e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 3.4772e-04 - accuracy: 1.0000 - val_loss: 3.3270e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.0792e-04 - accuracy: 1.0000 - val_loss: 3.4858e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 3.2058e-04 - accuracy: 1.0000 - val_loss: 3.5857e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.7365e-04 - accuracy: 1.0000 - val_loss: 2.1352e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.5910e-04 - accuracy: 1.0000 - val_loss: 1.9816e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 2.2309e-04 - accuracy: 1.0000 - val_loss: 2.4537e-04 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 2.0339e-04 - accuracy: 1.0000 - val_loss: 2.0669e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.8500e-04 - accuracy: 1.0000 - val_loss: 1.7083e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 1.7078e-04 - accuracy: 1.0000 - val_loss: 1.6922e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.5536e-04 - accuracy: 1.0000 - val_loss: 1.6255e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.4563e-04 - accuracy: 1.0000 - val_loss: 1.5993e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.3463e-04 - accuracy: 1.0000 - val_loss: 1.4013e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.2669e-04 - accuracy: 1.0000 - val_loss: 1.3156e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.2546e-04 - accuracy: 1.0000 - val_loss: 1.3453e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.1000e-04 - accuracy: 1.0000 - val_loss: 1.1844e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.0470e-04 - accuracy: 1.0000 - val_loss: 1.0624e-04 - val_accuracy: 1.0000\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.0549e-04 - accuracy: 1.0000\n",
      "Test Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prath\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set the path to your dataset\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "# Function to load and label the images\n",
    "def load_and_label_images(data_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for label, sign_folder in enumerate(os.listdir(data_dir)):\n",
    "        sign_folder_path = os.path.join(data_dir, sign_folder)\n",
    "\n",
    "        if os.path.isdir(sign_folder_path):\n",
    "            for img_file in os.listdir(sign_folder_path):\n",
    "                img_path = os.path.join(sign_folder_path, img_file)\n",
    "\n",
    "                # Check if the path is a file and ends with a valid image extension\n",
    "                if os.path.isfile(img_path) and img_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                    # Read the image\n",
    "                    img = cv2.imread(img_path)\n",
    "\n",
    "                    # Check if the image is loaded successfully\n",
    "                    if img is not None:\n",
    "                        # Resize the image\n",
    "                        img = cv2.resize(img, (64, 64))  # Adjust the size as needed\n",
    "\n",
    "                        # Append the image and label\n",
    "                        data.append(img)\n",
    "                        labels.append(label)\n",
    "                    else:\n",
    "                        print(f\"Error: Could not read the image {img_path}\")\n",
    "                else:\n",
    "                    print(f\"Skipping non-image file: {img_path}\")\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Load and label the images\n",
    "data, labels = load_and_label_images(DATA_DIR)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "label_encoder = LabelEncoder()\n",
    "labels_train_encoded = label_encoder.fit_transform(labels_train)\n",
    "labels_test_encoded = label_encoder.transform(labels_test)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "data_train_normalized = data_train / 255.0\n",
    "data_test_normalized = data_test / 255.0\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(data_train_normalized, labels_train_encoded, epochs=30, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(data_test_normalized, labels_test_encoded)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Save the trained model\n",
    "model.save('sign_language_cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b41cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6d887e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff00b66a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mflip(frame, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Define the region of interest\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m roi \u001b[38;5;241m=\u001b[39m frame[roi_top:roi_bottom, roi_left:roi_right]\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Preprocess the image (resize, flatten)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m roi \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(roi, (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the trained model\n",
    "with open('sign_language_model.pkl', 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# Define the region of interest (ROI) dimensions\n",
    "roi_top = 100\n",
    "roi_bottom = 300\n",
    "roi_left = 150\n",
    "roi_right = 350\n",
    "\n",
    "# Define a mapping between original class labels and display names\n",
    "class_mapping = {\n",
    "    0: 'Goodluck',\n",
    "    1: 'good job',\n",
    "    2: 'dislike',\n",
    "    3: 'you',\n",
    "    4: 'highfive',\n",
    "    5: 'namaste',\n",
    "    6: 'superb',\n",
    "    7: 'peace'\n",
    "}\n",
    "\n",
    "# Open a connection to the camera (0 is usually the default webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Flip the frame horizontally for a later selfie-view display\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Define the region of interest\n",
    "    roi = frame[roi_top:roi_bottom, roi_left:roi_right]\n",
    "\n",
    "    # Preprocess the image (resize, flatten)\n",
    "    roi = cv2.resize(roi, (64, 64))\n",
    "    roi_flattened = roi.flatten()\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict([roi_flattened])\n",
    "\n",
    "    # Map the original class label to the display name\n",
    "    display_name = class_mapping.get(prediction[0], 'Unknown')\n",
    "\n",
    "    # Display the prediction as text on the frame\n",
    "    cv2.putText(frame, f\"Prediction: {display_name}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Sign Language Prediction', frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571af7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
